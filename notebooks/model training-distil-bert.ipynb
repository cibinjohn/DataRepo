{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96968c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "# import spacy\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "# from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "\n",
    "from io import StringIO\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datetime import datetime\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    "    BertTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertForTokenClassification,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForTokenClassification,\n",
    "    DistilBertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForTokenClassification,\n",
    "    RobertaTokenizer,\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForTokenClassification,\n",
    "    XLMRobertaTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff2a9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224dc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a parser which is required for the pipeline\n",
    "def get_bert_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"The input data dir. Should contain the training files for the CoNLL-2003 NER task.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"Path to pre-trained model or shortcut name selected in the list: \",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        default=None,\n",
    "        type=str,\n",
    "        required=False,\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "\n",
    "    # Other parameters\n",
    "    parser.add_argument(\n",
    "        \"--labels\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Path to a file containing all labels. If not specified, CoNLL-2003 labels are used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cache_dir\",\n",
    "        default=\"\",\n",
    "        type=str,\n",
    "        help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_seq_length\",\n",
    "        default=128,\n",
    "        type=int,\n",
    "        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "             \"than this will be truncated, sequences shorter will be padded.\",\n",
    "    )\n",
    "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
    "    parser.add_argument(\n",
    "        \"--evaluate_during_training\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to run evaluation during training at each logging step.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_steps\",\n",
    "        default=-1,\n",
    "        type=int,\n",
    "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=50, help=\"Log every X updates steps.\")\n",
    "    parser.add_argument(\"--save_steps\", type=int, default=50, help=\"Save checkpoint every X updates steps.\")\n",
    "    parser.add_argument(\n",
    "        \"--eval_all_checkpoints\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    "    )\n",
    "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--fp16\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fp16_opt_level\",\n",
    "        type=str,\n",
    "        default=\"O1\",\n",
    "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "             \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef928413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure that the randomness in the training can be recreated\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82677cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe588c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7e26bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode=mode)\n",
    "\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation %s *****\", prefix)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n",
    "\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "    print(3)\n",
    "\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    results = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"precision\": precision_score(out_label_list, preds_list),\n",
    "        \"recall\": recall_score(out_label_list, preds_list),\n",
    "        \"f1\": f1_score(out_label_list, preds_list),\n",
    "    }\n",
    "    print(\"results : \",results)\n",
    "    logger.info(\"***** Eval results %s *****\", prefix)\n",
    "    for key in sorted(results.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(results[key]))\n",
    "\n",
    "    return results, preds_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68594470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples_from_file(data_dir, mode):\n",
    "    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index), words=words, labels=labels))\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                words.append(splits[0])\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"O\")\n",
    "        if words:\n",
    "            examples.append(InputExample(guid=\"%s-%d\".format(mode, guid_index), words=words, labels=labels))\n",
    "            \n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91442e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=1,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-100,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # print('word_tokens : ',word_tokens)\n",
    "            # print('label : ',label)\n",
    "            # print('label_map : ',label_map)\n",
    "\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[: (max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "        else:\n",
    "            input_ids += [pad_token] * padding_length\n",
    "            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
    "            segment_ids += [pad_token_segment_id] * padding_length\n",
    "            label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\", example.guid)\n",
    "            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids, label_ids=label_ids)\n",
    "        )\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4ce12f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, tokenizer, labels, pad_token_label_id, mode):\n",
    "    if args.local_rank not in [-1, 0] and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "    logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "    examples = read_examples_from_file(args.data_dir, mode)\n",
    "    features = convert_examples_to_features(\n",
    "        examples,\n",
    "        labels,\n",
    "        args.max_seq_length,\n",
    "        tokenizer,\n",
    "        cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n",
    "        # xlnet has a cls token at the end\n",
    "        cls_token=tokenizer.cls_token,\n",
    "        cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n",
    "        sep_token=tokenizer.sep_token,\n",
    "        sep_token_extra=bool(args.model_type in [\"roberta\"]),\n",
    "        # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n",
    "        pad_on_left=bool(args.model_type in [\"xlnet\"]),\n",
    "        # pad on the left for xlnet\n",
    "        pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "        pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n",
    "        pad_token_label_id=pad_token_label_id,\n",
    "    )\n",
    "\n",
    "    if args.local_rank == 0 and not evaluate:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94bec690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer, labels, pad_token_label_id):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if os.path.exists(args.model_name_or_path) and \"checkpoint\" in args.model_name_or_path:\n",
    "        # if os.path.exists(args.model_name_or_path):\n",
    "        # set global_step to gobal_step of last saved checkpoint from model path\n",
    "        global_step = int(args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0])\n",
    "        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "        logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "        logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "        logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "        logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproductibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n",
    "            if args.model_type != \"distilbert\":\n",
    "                inputs[\"token_type_ids\"] = (\n",
    "                    batch[2] if args.model_type in [\"bert\", \"xlnet\"] else None\n",
    "                )  # XLM and RoBERTa don\"t use segment_ids\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                            args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results, _ = evaluate(args, model, tokenizer, labels, pad_token_label_id, mode=\"dev\")\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd18cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTmodelTrain:\n",
    "    \"\"\"\n",
    "    Class for training educational attributes\n",
    "    \"\"\"\n",
    "    parser = get_bert_parser()\n",
    "\n",
    "    original_args = sys.argv\n",
    "    sys.argv = []\n",
    "    args = parser.parse_args()\n",
    "    sys.argv = original_args\n",
    "\n",
    "    args.model_type = 'distilbert'\n",
    "    args.model_name_or_path = 'distilbert-base-cased'\n",
    "    args.max_seq_length = 256\n",
    "    args.num_train_epochs = 6\n",
    "    args.per_gpu_train_batch_size = 4\n",
    "    args.save_steps = 750\n",
    "    args.seed = 1\n",
    "    # args.labels = 'data/labels.txt'\n",
    "    args.do_train = True\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    args.device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    # config_class, model_class, tokenizer_class = DistilBertConfig, DistilBertForTokenClassification, DistilBertTokenizer\n",
    "    config_class, model_class, tokenizer_class = BertConfig, BertForTokenClassification, BertTokenizer\n",
    "\n",
    "    # # Prepare CONLL-2003 task\n",
    "    # labels =['O', 'B-LOC', 'B-PER', 'B-ORG', 'I-PER', 'I-ORG', 'B-MISC', 'I-MISC', 'I-LOC']\n",
    "    # num_labels = len(labels)\n",
    "\n",
    "    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    def __init__(self, output_dir, labels, load_model_locally=None, epochs=None):\n",
    "        if epochs:\n",
    "            self.args.num_train_epochs = epochs\n",
    "        self.args.output_dir = output_dir\n",
    "        self.labels = labels\n",
    "        self.num_labels = len(self.labels)\n",
    "        if load_model_locally:\n",
    "            self.load_saved_model()\n",
    "        else:\n",
    "            # loading the pretrained model\n",
    "            self._load_pretrained_model()\n",
    "            \n",
    "    def load_saved_model(self):\n",
    "        \"\"\"\n",
    "        To load the model\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        labels_ = self.labels\n",
    "        pad_token_label_id_ = CrossEntropyLoss().ignore_index\n",
    "        # model_class, tokenizer_class = (DistilBertForTokenClassification, DistilBertTokenizer)\n",
    "        model_class, tokenizer_class = (BertForTokenClassification, BertTokenizer)\n",
    "\n",
    "        # if args.do_predict and args.local_rank in [-1, 0]:\n",
    "        print('\\n---Loading model...')\n",
    "        tm_s = time.time()\n",
    "\n",
    "        self.tokenizer = tokenizer_class.from_pretrained(self.args.output_dir, do_lower_case=self.args.do_lower_case)\n",
    "        self.model= model_class.from_pretrained(self.args.output_dir)\n",
    "        self.model.to(self.args.device)\n",
    "\n",
    "        tm_e = time.time()\n",
    "        print('---Model loaded. Time taken is %f seconds...', np.round(tm_e - tm_s, 2))\n",
    "\n",
    "\n",
    "    def _load_pretrained_model(self):\n",
    "        print('Loading pretrained model...')\n",
    "        ts = time.time()\n",
    "\n",
    "\n",
    "        self.config = BertConfig.from_pretrained(\n",
    "            self.args.config_name if self.args.config_name else self.args.model_name_or_path,\n",
    "            num_labels=self.num_labels,\n",
    "        )\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            self.args.tokenizer_name if self.args.tokenizer_name else self.args.model_name_or_path,\n",
    "            do_lower_case=self.args.do_lower_case,\n",
    "            cache_dir=self.args.cache_dir if self.args.cache_dir else None,\n",
    "        )\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            self.args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in self.args.model_name_or_path),\n",
    "            config=self.config,\n",
    "            cache_dir=self.args.cache_dir if self.args.cache_dir else None,\n",
    "        )\n",
    "\n",
    "        self.model.to(self.args.device)\n",
    "\n",
    "        te = time.time()\n",
    "        print('\\nPretrained model loaded.  Time taken is %f seconds...',np.round(te - ts, 2))\n",
    "\n",
    "    def _train(self, data_dir, dump_model=True):\n",
    "        self.args.data_dir = data_dir\n",
    "        train_dataset = load_and_cache_examples(self.args, self.tokenizer, self.labels, self.pad_token_label_id,\n",
    "                                                mode=\"train\")\n",
    "        train(self.args, train_dataset, self.model, self.tokenizer, self.labels, self.pad_token_label_id)\n",
    "\n",
    "        # save and upload the model to minio\n",
    "        if dump_model:\n",
    "            self._save_the_model()\n",
    "\n",
    "    def _save_the_model(self):\n",
    "\n",
    "        if os.path.exists(self.args.output_dir) and os.listdir(self.args.output_dir):\n",
    "            shutil.rmtree(self.args.output_dir)\n",
    "        if not os.path.exists(self.args.output_dir):\n",
    "            os.makedirs(self.args.output_dir)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(self.args.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(self.args, os.path.join(self.args.output_dir, \"training_args.bin\"))\n",
    "        \n",
    "    def _predict(self):\n",
    "        education_output = {}\n",
    "        print(\"\"\"os.path.join(self.args.data_dir, 'test.txt') \"\"\",os.path.join(self.args.data_dir, 'test.txt'))\n",
    "        file = open(os.path.join(self.args.data_dir, 'test.txt'), 'r')\n",
    "        actual_values_text = file.read()\n",
    "        file.close()\n",
    "\n",
    "        tokens_and_labels_list = actual_values_text.split('\\n')\n",
    "\n",
    "        try:\n",
    "            results, predictions = evaluate(self.args, self.model, self.tokenizer, self.labels, self.pad_token_label_id,\n",
    "                                      mode='test')\n",
    "            print(\"results : \",results)\n",
    "        except Exception as error_message:\n",
    "            print(\"In the evaluation part %s\", str(error_message))\n",
    "\n",
    "        try:\n",
    "            prediction_text = \"\"\n",
    "            sequence_id = 0\n",
    "            for token_and_label in tokens_and_labels_list:\n",
    "                if token_and_label.startswith(\"-DOCSTART-\") or token_and_label == \"\" or token_and_label == \"\\n\":\n",
    "                    prediction_text += '\\n'\n",
    "                    if not predictions[sequence_id]:\n",
    "                        sequence_id += 1\n",
    "                elif predictions[sequence_id]:\n",
    "                    output_line = token_and_label.split()[0] + \" \" + predictions[sequence_id].pop(0) + \"\\n\"\n",
    "                    prediction_text += output_line\n",
    "\n",
    "                else:\n",
    "                    # print(\"Maximum sequence length exceeded: No prediction for '%s'.\",\n",
    "                    #                   token_and_label.split()[0])\n",
    "                    output_line = token_and_label.split()[0] + \" O\\n\"\n",
    "                    prediction_text += output_line\n",
    "            prediction_text = prediction_text[:-1]\n",
    "        except Exception as error_message:\n",
    "            print(\"In prediction section\", str(error_message))\n",
    "\n",
    "        \n",
    "        return actual_values_text, prediction_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712f884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/cibin/Desktop/exl/TD/data/DS_v1/annotated/docanno_output/batch1/batch1_247/processed/final'\n",
    "save_dir = '/home/cibin/Desktop/exl/TD/models/transformers/model_batch1_distilbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3bb3cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>morning!</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27429</th>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27430</th>\n",
       "      <td>now</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27431</th>\n",
       "      <td>active.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27432</th>\n",
       "      <td>Customer:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27433</th>\n",
       "      <td>Perfect!</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27434 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token label\n",
       "0         Agent:     O\n",
       "1           Good     O\n",
       "2       morning!     O\n",
       "3           This     O\n",
       "4             is     O\n",
       "...          ...   ...\n",
       "27429         is     O\n",
       "27430        now     O\n",
       "27431    active.     O\n",
       "27432  Customer:     O\n",
       "27433   Perfect!     O\n",
       "\n",
       "[27434 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_resume_df = pd.read_csv(data_path+\"/train.txt\", sep=' ', header=None, names=['token','label'])\n",
    "test_resume_df = pd.read_csv(data_path+\"/test.txt\", sep=' ', header=None, names=['token','label'])\n",
    "\n",
    "train_resume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4051c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'Authentication']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = train_resume_df.label.unique().tolist()\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2574be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [token, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_resume_df[train_resume_df.label.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f94c3bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pretrained model loaded.  Time taken is %f seconds... 1.46\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# data_path = '/home/cibin/Desktop/exl/TD/data/DS_v1/annotated/docanno_output/sample_data/processed/final'\n",
    "# save_dir = '/home/cibin/Desktop/exl/TD/models/transformers/sample/v1'\n",
    "bert_obj = BERTmodelTrain(save_dir, labels=label_list, epochs=10)\n",
    "print(bert_obj.args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31c53962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cibin/virtual_envs/nlp_env/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|                                             | 0/10 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A/home/cibin/virtual_envs/nlp_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:32,  1.52it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:01<00:25,  1.88it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:23,  1.98it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:02<00:22,  2.04it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.08it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:20,  2.10it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:19,  2.12it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.13it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:18,  2.13it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.13it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:17,  2.13it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.13it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:16,  2.12it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.13it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:15,  2.13it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.13it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.13it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:14,  2.13it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.13it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:13,  2.13it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.12it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:10<00:12,  2.13it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.11it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:11<00:11,  2.11it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.11it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:12<00:10,  2.11it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.12it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:13<00:09,  2.12it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.12it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:14<00:08,  2.12it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.12it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.12it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.11it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.10it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.11it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.10it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.10it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.11it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:19<00:03,  2.11it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.11it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:20<00:02,  2.11it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.10it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:21<00:01,  2.09it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.09it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:22<00:00,  2.09it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.10it/s]\u001b[A/home/cibin/virtual_envs/nlp_env/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:271: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:23<00:00,  2.12it/s]\u001b[A\n",
      "Epoch:  10%|███▋                                 | 1/10 [00:23<03:32, 23.56s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:22,  2.15it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:22,  2.11it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.11it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:21,  2.10it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.09it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:20,  2.10it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.10it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.10it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.10it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.10it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.09it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.10it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.09it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.10it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.10it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.09it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.10it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.09it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:14,  2.09it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.09it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:13,  2.08it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.08it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:10<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.09it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:11<00:11,  2.10it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.09it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:12<00:10,  2.09it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.09it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:13<00:10,  2.09it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.09it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:14<00:09,  2.08it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.09it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.09it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.08it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.09it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.08it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.09it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.09it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.09it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.09it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.09it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.09it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.08it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:22<00:00,  2.08it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.08it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "Epoch:  20%|███████▍                             | 2/10 [00:47<03:09, 23.65s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.11it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:22,  2.09it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.09it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.08it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.08it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.09it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.09it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.09it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.08it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.08it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.08it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.08it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.08it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.08it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.09it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.08it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.08it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.09it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:14,  2.08it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.08it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.08it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:13<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.07it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:14<00:09,  2.08it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.08it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.07it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.08it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.08it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.08it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.08it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.08it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.07it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.07it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:23<00:00,  2.10it/s]\u001b[A\n",
      "Epoch:  30%|███████████                          | 3/10 [01:11<02:46, 23.74s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.08it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.06it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.07it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.07it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.08it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.07it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.08it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.08it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.08it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.07it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.07it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.08it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.07it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.07it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.07it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.07it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.06it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.07it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.07it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.07it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:14<00:09,  2.07it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.08it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.08it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.08it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.08it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.08it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.08it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:23<00:00,  2.09it/s]\u001b[A\n",
      "Epoch:  40%|██████████████▊                      | 4/10 [01:35<02:22, 23.81s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.08it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.05it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.07it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.06it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.07it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.07it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.07it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.07it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.07it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.07it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.07it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.07it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.07it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.07it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.07it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.08it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.08it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.07it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:14<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.07it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.06it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.06it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.06it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.07it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.07it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:23<00:00,  2.09it/s]\u001b[A\n",
      "Epoch:  50%|██████████████████▌                  | 5/10 [01:58<01:59, 23.86s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.08it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.06it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.05it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.05it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.05it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.05it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.05it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.05it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.05it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.06it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.07it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.07it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.08it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.07it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.07it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:15<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.07it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.07it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.06it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.06it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.06it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.06it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:24<00:00,  2.08it/s]\u001b[A\n",
      "Epoch:  60%|██████████████████████▏              | 6/10 [02:22<01:35, 23.91s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.07it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.06it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.06it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.06it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.06it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:18,  2.05it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.05it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.05it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:16,  2.05it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.05it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:15,  2.05it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.06it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.06it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.06it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.05it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.05it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:15<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.07it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:16<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.05it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:24<00:00,  2.08it/s]\u001b[A\n",
      "Epoch:  70%|█████████████████████████▉           | 7/10 [02:47<01:11, 23.95s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.07it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.07it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.07it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.07it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.07it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.07it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.06it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.06it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.07it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.07it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.07it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.06it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.06it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:15<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.07it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.06it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.06it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.06it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.07it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.07it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:24<00:00,  2.08it/s]\u001b[A\n",
      "Epoch:  80%|█████████████████████████████▌       | 8/10 [03:11<00:47, 23.97s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.09it/s]\u001b[A\n",
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.07it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.07it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.07it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.07it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.07it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.07it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.07it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.07it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.07it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:15,  2.06it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.05it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.05it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.05it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.06it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.07it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:15<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:15<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.05it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.05it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.05it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.05it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.06it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.06it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.07it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:24<00:00,  2.08it/s]\u001b[A\n",
      "Epoch:  90%|█████████████████████████████████▎   | 9/10 [03:35<00:23, 23.99s/it]\n",
      "Iteration:   0%|                                         | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   2%|▋                                | 1/50 [00:00<00:23,  2.12it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   4%|█▎                               | 2/50 [00:00<00:23,  2.07it/s]\u001b[A\n",
      "Iteration:   6%|█▉                               | 3/50 [00:01<00:22,  2.07it/s]\u001b[A\n",
      "Iteration:   8%|██▋                              | 4/50 [00:01<00:22,  2.06it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 5/50 [00:02<00:21,  2.06it/s]\u001b[A\n",
      "Iteration:  12%|███▉                             | 6/50 [00:02<00:21,  2.06it/s]\u001b[A\n",
      "Iteration:  14%|████▌                            | 7/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      "Iteration:  16%|█████▎                           | 8/50 [00:03<00:20,  2.05it/s]\u001b[A\n",
      "Iteration:  18%|█████▉                           | 9/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 10/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      "Iteration:  22%|███████                         | 11/50 [00:05<00:18,  2.06it/s]\u001b[A\n",
      "Iteration:  24%|███████▋                        | 12/50 [00:05<00:18,  2.05it/s]\u001b[A\n",
      "Iteration:  26%|████████▎                       | 13/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  28%|████████▉                       | 14/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 15/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  32%|██████████▏                     | 16/50 [00:07<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  34%|██████████▉                     | 17/50 [00:08<00:16,  2.06it/s]\u001b[A\n",
      "Iteration:  36%|███████████▌                    | 18/50 [00:08<00:15,  2.05it/s]\u001b[A\n",
      "Iteration:  38%|████████████▏                   | 19/50 [00:09<00:15,  2.05it/s]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  42%|█████████████▍                  | 21/50 [00:10<00:14,  2.06it/s]\u001b[A\n",
      "Iteration:  44%|██████████████                  | 22/50 [00:10<00:13,  2.06it/s]\u001b[A\n",
      "Iteration:  46%|██████████████▋                 | 23/50 [00:11<00:13,  2.06it/s]\u001b[A\n",
      "Iteration:  48%|███████████████▎                | 24/50 [00:11<00:12,  2.06it/s]\u001b[A\n",
      "Iteration:  50%|████████████████                | 25/50 [00:12<00:12,  2.06it/s]\u001b[A\n",
      "Iteration:  52%|████████████████▋               | 26/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
      "Iteration:  54%|█████████████████▎              | 27/50 [00:13<00:11,  2.06it/s]\u001b[A\n",
      "Iteration:  56%|█████████████████▉              | 28/50 [00:13<00:10,  2.05it/s]\u001b[A\n",
      "Iteration:  58%|██████████████████▌             | 29/50 [00:14<00:10,  2.06it/s]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 30/50 [00:14<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  62%|███████████████████▊            | 31/50 [00:15<00:09,  2.06it/s]\u001b[A\n",
      "Iteration:  64%|████████████████████▍           | 32/50 [00:15<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  66%|█████████████████████           | 33/50 [00:16<00:08,  2.06it/s]\u001b[A\n",
      "Iteration:  68%|█████████████████████▊          | 34/50 [00:16<00:07,  2.06it/s]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 35/50 [00:16<00:07,  2.06it/s]\u001b[A\n",
      "Iteration:  72%|███████████████████████         | 36/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  74%|███████████████████████▋        | 37/50 [00:17<00:06,  2.07it/s]\u001b[A\n",
      "Iteration:  76%|████████████████████████▎       | 38/50 [00:18<00:05,  2.07it/s]\u001b[A\n",
      "Iteration:  78%|████████████████████████▉       | 39/50 [00:18<00:05,  2.06it/s]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 40/50 [00:19<00:04,  2.05it/s]\u001b[A\n",
      "Iteration:  82%|██████████████████████████▏     | 41/50 [00:19<00:04,  2.06it/s]\u001b[A\n",
      "Iteration:  84%|██████████████████████████▉     | 42/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      "Iteration:  86%|███████████████████████████▌    | 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      "Iteration:  88%|████████████████████████████▏   | 44/50 [00:21<00:02,  2.06it/s]\u001b[A\n",
      "Iteration:  90%|████████████████████████████▊   | 45/50 [00:21<00:02,  2.05it/s]\u001b[A\n",
      "Iteration:  92%|█████████████████████████████▍  | 46/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  94%|██████████████████████████████  | 47/50 [00:22<00:01,  2.06it/s]\u001b[A\n",
      "Iteration:  96%|██████████████████████████████▋ | 48/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration:  98%|███████████████████████████████▎| 49/50 [00:23<00:00,  2.06it/s]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 50/50 [00:24<00:00,  2.08it/s]\u001b[A\n",
      "Epoch: 100%|████████████████████████████████████| 10/10 [03:59<00:00, 23.91s/it]\n"
     ]
    }
   ],
   "source": [
    "bert_obj._train(\n",
    "        data_dir=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f77f89bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os.path.join(self.args.data_dir, 'test.txt')  /home/cibin/Desktop/exl/TD/data/DS_v1/annotated/docanno_output/batch1/batch1_247/processed/final/test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████| 7/7 [00:02<00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "results :  {'loss': 0.1978007054754666, 'precision': 0.049019607843137254, 'recall': 0.2127659574468085, 'f1': 0.07968127490039839}\n",
      "results :  {'loss': 0.1978007054754666, 'precision': 0.049019607843137254, 'recall': 0.2127659574468085, 'f1': 0.07968127490039839}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/cibin/virtual_envs/nlp_env/lib/python3.11/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Authentication seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    }
   ],
   "source": [
    "actual_values, predictions = bert_obj._predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a47528b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maple</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bank's</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retirement</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Planning,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>scam.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>Customer:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>Good</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>to</td>\n",
       "      <td>Authentication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>know!</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6684 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token           label\n",
       "0         Agent:               O\n",
       "1          Maple               O\n",
       "2         Bank's               O\n",
       "3     Retirement               O\n",
       "4      Planning,               O\n",
       "...          ...             ...\n",
       "6679       scam.               O\n",
       "6680   Customer:               O\n",
       "6681        Good               O\n",
       "6682          to  Authentication\n",
       "6683       know!               O\n",
       "\n",
       "[6684 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.read_csv(StringIO(predictions), sep=' ', names=['token','label'])\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eede9c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"distil_predictions.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17ae1bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Agent:</strong> Maple Bank's Retirement Planning, this is Nicole speaking. How can I help? </div><div><strong>Customer:</strong> Hello Nicole, I'm 55 and considering early retirement next year. </div><div><strong>Agent:</strong> That's a major life transition that requires careful planning. First, let's discuss your retirement accounts <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">-</span> do <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">you</span> have a 401(k), IRA, or <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">other</span> savings? </div><div><strong>Customer:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Yes,</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">$800,000</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">in</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">my</span> 401(k) <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">and</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">$200,000</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">in</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">IRAs.</span> </div><div><strong>Agent:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">At</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">your</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">age,</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">you</span> can access 401(k) funds without penalty if <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">you</span> retire, but I'd recommend rolling <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">it</span> to an IRA first for more flexibility. We should analyze your Social Security strategy, healthcare coverage, and tax-efficient withdrawal plan. Would you like to schedule a comprehensive retirement review with our CFP®? </div><div><strong>Customer:</strong> Yes, what information should I prepare? </div><div><strong>Agent:</strong> Please gather statements for all accounts, your most recent tax return, and a list of expected retirement expenses. I'm scheduling you with Mr. Reynolds, our senior planner, who specializes in early retirement transitions. </div><div><strong>Agent:</strong> Good morning! This is Maple Bank's Home Equity department. My name is Robert. How can I help? </div><div><strong>Customer:</strong> Hi Robert, I want to use my home equity to consolidate high-interest credit card debt. </div><div><strong>Agent:</strong> Smart financial move, Mr. Garcia. Let's explore options. <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">First,</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">could</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">you</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">verify</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">your</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">property</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">address</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">and</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">loan</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">number?</span> </div><div><strong>Customer:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">789</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Oak</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Street,</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">loan</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">number</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">987654.</span> </div><div><strong>Agent:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Thank</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">you.</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">For</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">security,</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">I'll</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">ask</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">about</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">your</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">last</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">mortgage</span> </div><div><strong>payment:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">What</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">was</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">the</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">amount</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">and</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">date?</span> </div><div><strong>Customer:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">$1,450</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">on</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">June</span> 1st. </div><div><strong>Agent:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Verified.</span> You have about $85,000 in available equity. Our HELOC </div><div><strong>offers:</strong> 1) 5.75% variable APR, 2) Interest-only payments for first 10 years, and 3) No closing costs. Would you like to proceed? </div><div><strong>Customer:</strong> Yes. What's the maximum I can borrow? </div><div><strong>Agent:</strong> Up to 80% of your home's value minus current mortgage. For you, that's $125,000. I recommend borrowing only what you need to pay off the cards. </div><div><strong>Customer:</strong> I'll take $60,000 then. </div><div><strong>Agent:</strong> Wise decision. I've emailed the application. A notary will contact you to sign documents at home. Funds will be available in 10-14 days. </div><div><strong>Customer:</strong> Thank you for your help, Robert. </div><div><strong>Agent:</strong> You're very welcome, Mr. Garcia. I've also included a debt payoff calculator in the email. </div><div><strong>Agent:</strong> Maple Bank support, this is Carlos. How may I assist? </div><div><strong>Customer:</strong> I need to activate my new debit card. </div><div><strong>Agent:</strong> Sure. Confirm <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">the</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">last</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">four</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">digits</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">of</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">your</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">SSN.</span> </div><div><strong>Customer:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">4321.</span> </div><div><strong>Agent:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Thanks.</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Now,</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">what’s</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">your</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">birth</span> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">year?</span> </div><div><strong>Customer:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">1985.</span> </div><div><strong>Agent:</strong> <span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"Authentication\">Verified.</span> Your card </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def html_ner_visualization(predictions_df):\n",
    "    html = []\n",
    "    current_speaker = None\n",
    "    \n",
    "    for _, row in predictions_df.iterrows():\n",
    "        token = row['token']\n",
    "        label = row['label']\n",
    "        \n",
    "        if token.endswith(':'):\n",
    "            if current_speaker:\n",
    "                html.append('</div>')\n",
    "            current_speaker = token\n",
    "            html.append(f'<div><strong>{token}</strong> ')\n",
    "        else:\n",
    "            if label != 'O':\n",
    "                html.append(f'<span style=\"background-color: #ffcccc; border-radius: 3px;\" title=\"{label}\">{token}</span> ')\n",
    "            else:\n",
    "                html.append(f'{token} ')\n",
    "    \n",
    "    if current_speaker:\n",
    "        html.append('</div>')\n",
    "    \n",
    "    display(HTML(''.join(html)))\n",
    "\n",
    "html_ner_visualization(predictions_df.head(400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bba18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
